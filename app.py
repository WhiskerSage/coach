# å…ˆåˆ‡æ¢ç«¯å£
# set HTTPS_PROXY=http://127.0.0.1:7897 ä¸€å®šè¦åœ¨cmdé‡Œåˆ‡æ¢åˆ°ä½ vpnçš„ç«¯å£ï¼Œä¸ç„¶è¿ä¸ä¸Šã€‚ä¸€å®šè¦åœ¨cmdé‡Œï¼ï¼ï¼
# streamlit run app.py æ¥å¯åŠ¨
# --- æœ€ç»ˆä¼˜åŒ–ä¸ä¿®æ­£ç‰ˆæœ¬ v4 ---

import streamlit as st
import cv2
import mediapipe as mp
import tempfile
import time
from PIL import Image
import io
import numpy as np
import pandas as pd
import plotly.graph_objs as go
import re
import base64
from langchain_google_genai import ChatGoogleGenerativeAI, HarmCategory, HarmBlockThreshold
from langchain_core.messages import HumanMessage, AIMessage
from langchain_community.document_loaders import DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# --- é¡µé¢é…ç½®å’Œæ ‡é¢˜ ---
st.set_page_config(
    page_title="AIè¿åŠ¨æ•™ç»ƒ Demo",
    page_icon="ğŸ¤–",
    layout="wide"
)
st.title("ğŸ¤– AI è¿åŠ¨æ•™ç»ƒ")
st.caption("ä¸Šä¼ ä¸€æ®µè¿åŠ¨è§†é¢‘ï¼Œè®©AIä¸ºä½ åˆ†æå§¿æ€ã€‚æ”¯æŒè¿ç»­å¯¹è¯ã€‚")


# --- Gemini API é…ç½® (è‡ªåŠ¨ä» secrets è¯»å–) ---
api_key = st.secrets.get('GEMINI_API_KEY', None)
if not api_key:
    st.error("æœªæ£€æµ‹åˆ° Gemini API å¯†é’¥ï¼Œè¯·åœ¨ .streamlit/secrets.toml ä¸­é…ç½® GEMINI_API_KEYã€‚")
    st.stop()

# --- å®šä¹‰å®‰å…¨è®¾ç½® ---
safety_settings = {
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
}

# --- RAG Setup: åˆ›å»ºå¹¶ç¼“å­˜Retriever ---
@st.cache_resource
def get_retriever(api_key):
    try:
        loader = DirectoryLoader('./knowledge_base/', glob="**/*.md", show_progress=True)
        documents = loader.load()
        if not documents:
            st.warning("çŸ¥è¯†åº“ä¸ºç©ºï¼ŒRAGåŠŸèƒ½å°†ä¸ä¼šç”Ÿæ•ˆã€‚è¯·åœ¨ knowledge_base æ–‡ä»¶å¤¹ä¸­æ·»åŠ Markdownæ–‡ä»¶ã€‚")
            return None
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        texts = text_splitter.split_documents(documents)
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
        vectorstore = FAISS.from_documents(texts, embeddings)
        return vectorstore.as_retriever()
    except Exception as e:
        st.error(f"åˆ›å»ºRAGç´¢å¼•æ—¶å‡ºé”™: {e}")
        return None

# --- LangChain åˆå§‹åŒ– ---
try:
    if "llm" not in st.session_state:
        st.session_state.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            google_api_key=api_key,
            safety_settings=safety_settings,
        )
    if "history" not in st.session_state:
        st.session_state.history = []  # LangChainçš„æ¶ˆæ¯å†å²
    if "analysis_df" not in st.session_state:
        st.session_state.analysis_df = pd.DataFrame() # ç”¨äºå­˜å‚¨åˆ†ææ•°æ®
    if "retriever" not in st.session_state:
        st.session_state.retriever = get_retriever(api_key) # åˆå§‹åŒ–RAG
except Exception as e:
    st.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥API Keyå’Œç½‘ç»œä»£ç†ã€‚é”™è¯¯: {e}")
    st.stop()


# --- ä¾§è¾¹æ  UI ---
with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶é¢æ¿") # ä¼˜åŒ–ç‚¹ï¼šä½¿ç”¨æ›´é€šç”¨çš„å›¾æ ‡
    
    # --- ä¼˜åŒ–ç‚¹ï¼šæ”¹è¿›"æ–°å»ºå¯¹è¯"äº¤äº’ ---
    if st.button("âœ¨ æ–°å»ºå¯¹è¯", use_container_width=True):
        st.session_state.history = []
        st.session_state.analysis_df = pd.DataFrame()
        st.success("æ–°çš„å¯¹è¯å·²å¼€å§‹ï¼")
        time.sleep(0.5) # çŸ­æš‚æ˜¾ç¤ºæˆåŠŸä¿¡æ¯ï¼Œç„¶ååˆ·æ–°
        st.rerun()

    # --- é“œç‰ŒåŠŸèƒ½ï¼šå¢åŠ ç”¨æˆ·ç›®æ ‡è¾“å…¥æ¡† ---
    user_goal = st.text_input("æˆ‘çš„è®­ç»ƒç›®æ ‡:", placeholder="ä¾‹å¦‚ï¼šæ”¹å–„æ·±è¹²æ—¶è†ç›–å†…æ‰£")
    st.divider()

    # --- ä¼˜åŒ–ç‚¹ï¼šå¢åŠ ç”¨æˆ·å¼•å¯¼ ---
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ ä½ çš„è¿åŠ¨è§†é¢‘",
        type=["mp4", "mov", "avi"],
        help="å»ºè®®ä¸Šä¼ 5-15ç§’çš„çŸ­è§†é¢‘ï¼Œä»¥è·å¾—æœ€ä½³åˆ†æé€Ÿåº¦å’Œæ•ˆæœã€‚"
    )
    
    desired_frames = st.slider(
        "åˆ†æå¼ºåº¦ (å¸§æ•°)", # ä¼˜åŒ–ç‚¹ï¼šæ ‡ç­¾æ›´æ˜“æ‡‚
        min_value=2, max_value=10, value=6, step=1,
        help="é€‰æ‹©ä»è§†é¢‘ä¸­æŠ½å–çš„å…³é”®ç”»é¢æ•°é‡ã€‚æ•°é‡è¶Šå¤šï¼Œåˆ†æè¶Šç²¾ç»†ï¼Œä½†å¤„ç†æ—¶é—´ä¹Ÿæ›´é•¿ã€‚"
    )
    
    analyze_button = st.button("å¼€å§‹åˆ†æ", use_container_width=True, disabled=not uploaded_file)


# --- ä¸»èŠå¤©ç•Œé¢ ---

# --- ä¼˜åŒ–ç‚¹ï¼šå¢åŠ æ¬¢è¿é¡µ/å¼•å¯¼åŒºï¼Œé¿å…å†·å¯åŠ¨ ---
if not st.session_state.history:
    st.markdown("""
        <div style="text-align: center; padding: 2rem 1rem;">
            <h2 style="font-weight: bold;">æ¬¢è¿ä½¿ç”¨ AI è¿åŠ¨æ•™ç»ƒ</h2>
            <p>æˆ‘æ˜¯æ‚¨çš„ä¸“å±AIæ•™ç»ƒï¼Œå¯ä»¥åˆ†ææ‚¨ä¸Šä¼ çš„è¿åŠ¨è§†é¢‘ï¼Œæä¾›ä¸“ä¸šçš„å§¿æ€è¯„ä¼°å’Œæ”¹è¿›å»ºè®®ã€‚</p>
            <p><strong>è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å¼€å§‹ï¼š</strong></p>
            <ol style="display: inline-block; text-align: left; margin-top: 1rem;">
                <li>åœ¨å·¦ä¾§çš„ <strong>æ§åˆ¶é¢æ¿</strong> ä¸Šä¼ æ‚¨çš„è¿åŠ¨è§†é¢‘ã€‚</li>
                <li>ï¼ˆå¯é€‰ï¼‰è°ƒæ•´æ‚¨å¸Œæœ›åˆ†æçš„ <strong>å…³é”®å¸§æ•°é‡</strong>ã€‚</li>
                <li>ç‚¹å‡» <strong>"å¼€å§‹åˆ†æ"</strong> æŒ‰é’®ï¼Œç¨ç­‰ç‰‡åˆ»å³å¯è·å¾—æŠ¥å‘Šã€‚</li>
            </ol>
            <p>æœŸå¾…çœ‹åˆ°æ‚¨çš„ç²¾å½©è¡¨ç°ï¼</p>
        </div>
    """, unsafe_allow_html=True)

# æ˜¾ç¤ºå†å²å¯¹è¯è®°å½•
for message in st.session_state.history:
    # ä¸æ˜¾ç¤ºåˆå§‹çš„å¤šæ¨¡æ€ç”¨æˆ·æ¶ˆæ¯ï¼Œåªæ˜¾ç¤ºAIå›å¤å’Œåç»­æ–‡æœ¬å¯¹è¯
    if isinstance(message, AIMessage):
        with st.chat_message("assistant"):
            st.markdown(message.content, unsafe_allow_html=True)
    elif isinstance(message, HumanMessage) and isinstance(message.content, str):
         with st.chat_message("user"):
            st.markdown(message.content, unsafe_allow_html=True)

# --- é‡‘ç‰ŒåŠŸèƒ½ï¼šæ–°å¢è§’åº¦è®¡ç®—å‡½æ•° ---
def calculate_angle(a, b, c):
    """è®¡ç®—ç”±ä¸‰ç‚¹a, b, cæ„æˆçš„è§’åº¦ï¼ˆbä¸ºé¡¶ç‚¹ï¼‰ï¼Œè¿”å›0-180ä¹‹é—´çš„è§’åº¦å€¼"""
    a = np.array(a)
    b = np.array(b)
    c = np.array(c)
    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])
    angle = np.abs(radians * 180.0 / np.pi)
    if angle > 180.0:
        angle = 360 - angle
    return angle

# --- æ ¸å¿ƒå¤„ç†é€»è¾‘ ---
if analyze_button:
    # --- é‡‘ç‰ŒåŠŸèƒ½ï¼šåˆå§‹åŒ–ç”¨äºå­˜å‚¨é‡åŒ–æ•°æ®çš„å­—å…¸ ---
    quantitative_data = {
        "å¸§å·": [],
        "å·¦è†è§’åº¦": [],
        "å³è†è§’åº¦": [],
        "å·¦é«‹è§’åº¦": [],
        "å³é«‹è§’åº¦": []
    }
    with st.spinner("å¤„ç†ä¸­ï¼Œè¯·ç¨å€™..."):
        st.info("AIæ­£åœ¨åˆ†æ...è¯·ç¨å€™")
        # --- è§†é¢‘å¤„ç†é€»è¾‘ ---
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tfile:
            tfile.write(uploaded_file.read())
            video_path = tfile.name
        
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if total_frames == 0:
            st.error("æ— æ³•è¯»å–è§†é¢‘ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æŸåã€‚")
            st.stop()
        
        frame_interval = max(total_frames // desired_frames, 1)
        st.write(f"è§†é¢‘æ€»å¸§æ•°: {total_frames}ï¼Œå°†å‡åŒ€æŠ½å– {desired_frames} å¸§è¿›è¡Œåˆ†æã€‚")
        
        sampled_frames_pil = []
        frame_indices_to_extract = [i * frame_interval for i in range(desired_frames)]
        
        with mp.solutions.pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:
            for frame_index in frame_indices_to_extract:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
                ret, frame = cap.read()
                if not ret:
                    continue
                
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = pose.process(rgb_frame)
                annotated_image = frame.copy()
                
                if results.pose_landmarks:
                    mp.solutions.drawing_utils.draw_landmarks(
                        annotated_image, results.pose_landmarks, mp.solutions.pose.POSE_CONNECTIONS,
                        landmark_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),
                        connection_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)
                    )
                
                # --- é‡‘ç‰ŒåŠŸèƒ½ï¼šè®¡ç®—è§’åº¦å¹¶å­˜å‚¨ ---
                try:
                    landmarks = results.pose_landmarks.landmark
                    left_hip = [landmarks[mp.solutions.pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp.solutions.pose.PoseLandmark.LEFT_HIP.value].y]
                    left_knee = [landmarks[mp.solutions.pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp.solutions.pose.PoseLandmark.LEFT_KNEE.value].y]
                    left_ankle = [landmarks[mp.solutions.pose.PoseLandmark.LEFT_ANKLE.value].x, landmarks[mp.solutions.pose.PoseLandmark.LEFT_ANKLE.value].y]
                    left_shoulder = [landmarks[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER.value].y]
                    right_hip = [landmarks[mp.solutions.pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp.solutions.pose.PoseLandmark.RIGHT_HIP.value].y]
                    right_knee = [landmarks[mp.solutions.pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp.solutions.pose.PoseLandmark.RIGHT_KNEE.value].y]
                    right_ankle = [landmarks[mp.solutions.pose.PoseLandmark.RIGHT_ANKLE.value].x, landmarks[mp.solutions.pose.PoseLandmark.RIGHT_ANKLE.value].y]
                    right_shoulder = [landmarks[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER.value].y]
                    lk_angle = calculate_angle(left_hip, left_knee, left_ankle)
                    rk_angle = calculate_angle(right_hip, right_knee, right_ankle)
                    lh_angle = calculate_angle(left_shoulder, left_hip, left_knee)
                    rh_angle = calculate_angle(right_shoulder, right_hip, right_knee)
                    quantitative_data["å¸§å·"].append(frame_index)
                    quantitative_data["å·¦è†è§’åº¦"].append(lk_angle)
                    quantitative_data["å³è†è§’åº¦"].append(rk_angle)
                    quantitative_data["å·¦é«‹è§’åº¦"].append(lh_angle)
                    quantitative_data["å³é«‹è§’åº¦"].append(rh_angle)
                except Exception as e:
                    print(f"åœ¨å¸§ {frame_index} è®¡ç®—è§’åº¦æ—¶å‡ºé”™: {e}")
                    pass
                
                pil_image = Image.fromarray(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))
                sampled_frames_pil.append(pil_image)
        
        cap.release()
        st.write(f"å›¾åƒæå–å®Œæˆï¼å…±æå– {len(sampled_frames_pil)} å¸§ã€‚æ­£åœ¨å‡†å¤‡å‘é€...")

        if not sampled_frames_pil:
            st.error("æ— æ³•ä»è§†é¢‘ä¸­æå–ä»»ä½•æœ‰æ•ˆå¸§ã€‚")
        else:
            st.session_state.analysis_df = pd.DataFrame(quantitative_data)
            df = st.session_state.analysis_df
            # --- åä¸½å…³é”®å¸§æ¨ªå‘å¤§å›¾å±•ç¤º ---
            st.markdown("#### å…³é”®å¸§é¢„è§ˆ")
            cols = st.columns(len(sampled_frames_pil))
            for i, img in enumerate(sampled_frames_pil):
                with cols[i]:
                    st.image(img, caption=f"å¸§ {i+1}", use_container_width=True)

            # --- LangChain Prompt & Invocation ---
            focus_prompt = f"ç”¨æˆ·çš„è®­ç»ƒç›®æ ‡æ˜¯{user_goal}ã€‚å¦‚æœ‰ç›¸å…³é—®é¢˜è¯·é€‚å½“å…³æ³¨ã€‚" if user_goal else ""
            data_prompt = f"\nä»¥ä¸‹ä¸ºéƒ¨åˆ†å¸§çš„é‡åŒ–æ•°æ®ï¼Œä»…ä¾›ä½ åˆ†ææ—¶å‚è€ƒï¼Œé‡ç‚¹è¯·ç»“åˆè§†é¢‘å¸§çš„å¤šæ¨¡æ€ç†è§£è¿›è¡Œç»¼åˆåˆ¤æ–­ï¼š\n{df.to_markdown(index=False)}\n" if not df.empty else ""
            prompt_text = f"""
            ä½ æ˜¯ä¸€ä½é¡¶çº§çš„è¿åŠ¨ç”Ÿç‰©åŠ›å­¦ä¸“å®¶å’ŒAIæ•™ç»ƒã€‚ä½ çš„æ‰€æœ‰å›ç­”éƒ½å¿…é¡»ä½¿ç”¨ç®€ä½“ä¸­æ–‡ã€‚
            ä½ çš„ä»»åŠ¡æ˜¯åŸºäºç”¨æˆ·ä¸Šä¼ çš„è§†é¢‘å¸§ä¸ºä¸»ï¼Œç»“åˆéƒ¨åˆ†é‡åŒ–æ•°æ®ï¼ˆä»…ä½œè¾…åŠ©å‚è€ƒï¼‰ï¼Œæä¾›ä¸€ä»½ä¸“ä¸šã€æ·±å…¥ã€ä»¥å¤šæ¨¡æ€ç†è§£ä¸ºæ ¸å¿ƒçš„åˆ†ææŠ¥å‘Šã€‚
            {focus_prompt}
            {data_prompt}

            **è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š**
            è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ä¸‰ä¸ªéƒ¨åˆ†è¿›è¡Œç»„ç»‡ï¼š

            - **ã€ç»¼åˆè¯„ä¼°ä¸å¾—åˆ†ã€‘**: 
              é¦–å…ˆï¼Œ**å¿…é¡»ä½¿ç”¨Markdownè¡¨æ ¼**æ¸…æ™°åœ°å±•ç¤ºå››ä¸ªç»´åº¦çš„å¾—åˆ†ã€‚è¡¨æ ¼åº”åŒ…å«"è¯„ä¼°ç»´åº¦"å’Œ"å¾—åˆ† (æ»¡åˆ†10)"ä¸¤åˆ—ã€‚
              ç„¶åï¼Œåœ¨è¡¨æ ¼ä¸‹æ–¹ç»™å‡ºä¸€ä¸ªç®€çŸ­çš„æ€»ä½“è¯„ä»·ã€‚

            - **ã€å¤šæ¨¡æ€è¯Šæ–­ã€‘**: 
              è¿™æ˜¯æŠ¥å‘Šçš„æ ¸å¿ƒã€‚è¯·**ä»¥è§†é¢‘å¸§çš„å¤šæ¨¡æ€ç†è§£ä¸ºä¸»ï¼Œé‡åŒ–æ•°æ®ä»…ä½œè¾…åŠ©**ï¼Œé€é¡¹è§£é‡Šæ‰“åˆ†ä¾æ®ã€‚
              ä¾‹å¦‚ï¼š"åœ¨ç¬¬Xå¸§å›¾åƒä¸­è§‚å¯Ÿåˆ°..."ã€‚ä¸è¦åªå›´ç»•é‡åŒ–æ•°æ®å±•å¼€ã€‚

            - **ã€æ ¸å¿ƒæ”¹è¿›å»ºè®®ã€‘**: 
              é’ˆå¯¹å¾—åˆ†è¾ƒä½çš„ç»´åº¦å’Œç”¨æˆ·ç›®æ ‡ï¼Œæä¾›æœ€å…³é”®ã€æœ€å¯æ“ä½œçš„è®­ç»ƒå»ºè®®ã€‚

            ä½ çš„è¯­æ°”åº”ä¸“ä¸šã€ä¸¥è°¨ä¸”å¯Œæœ‰é¼“åŠ±æ€§ã€‚è¯·å¼€å§‹åˆ†æã€‚
            """
            
            # å°†PILå›¾åƒè½¬æ¢ä¸ºLangChainæ‰€éœ€æ ¼å¼
            image_messages = []
            for img in sampled_frames_pil:
                img.thumbnail((768, 768))
                buf = io.BytesIO()
                img.save(buf, format='JPEG')
                base64_image = base64.b64encode(buf.getvalue()).decode('utf-8')
                image_messages.append({
                    "type": "image_url",
                    "image_url": f"data:image/jpeg;base64,{base64_image}"
                })

            # åˆ›å»ºLangChainçš„å¤šæ¨¡æ€æ¶ˆæ¯
            user_message = HumanMessage(
                content=[{"type": "text", "text": prompt_text}] + image_messages
            )
            
            # æ¸…ç©ºå†å²ï¼Œå¼€å§‹æ–°çš„åˆ†æä¼šè¯
            st.session_state.history = []

            try:
                st.info("AIå¤§æ¨¡å‹æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
                with st.chat_message("assistant"):
                    response_container = st.empty()
                    collected_messages = ""
                    # ä½¿ç”¨LangChain LLMè¿›è¡Œæµå¼è°ƒç”¨
                    response = st.session_state.llm.stream([user_message])
                    for chunk in response:
                        if chunk.content:
                            collected_messages += chunk.content
                            response_container.markdown(collected_messages, unsafe_allow_html=True)

                    # å°†AIçš„å®Œæ•´å›å¤å­˜å…¥å†å²è®°å½•
                    st.session_state.history.append(AIMessage(content=collected_messages))

                    # --- åˆ†æå®Œæˆåçš„å›¾è¡¨å’Œä¸‹è½½æŒ‰é’® ---
                    if not df.empty:
                        with st.expander("ğŸ“ˆ è¯¦ç»†æ•°æ®å›¾è¡¨", expanded=True):
                            # è†å…³èŠ‚è§’åº¦å˜åŒ–
                            fig_knee = go.Figure()
                            fig_knee.add_trace(go.Scatter(x=df['å¸§å·'], y=df['å·¦è†è§’åº¦'], mode='lines+markers', name='å·¦è†', line=dict(color='red', width=4), marker=dict(size=10)))
                            fig_knee.add_trace(go.Scatter(x=df['å¸§å·'], y=df['å³è†è§’åº¦'], mode='lines+markers', name='å³è†', line=dict(color='blue', width=4), marker=dict(size=10)))
                            fig_knee.update_layout(title='è†å…³èŠ‚è§’åº¦å˜åŒ–', xaxis_title='å¸§å·', yaxis_title='è§’åº¦ (Â°)', template='plotly_dark')
                            st.plotly_chart(fig_knee, use_container_width=True)
                            # é«‹å…³èŠ‚è§’åº¦å˜åŒ–
                            fig_hip = go.Figure()
                            fig_hip.add_trace(go.Scatter(x=df['å¸§å·'], y=df['å·¦é«‹è§’åº¦'], mode='lines+markers', name='å·¦é«‹', line=dict(color='orange', width=4), marker=dict(size=10)))
                            fig_hip.add_trace(go.Scatter(x=df['å¸§å·'], y=df['å³é«‹è§’åº¦'], mode='lines+markers', name='å³é«‹', line=dict(color='green', width=4), marker=dict(size=10)))
                            fig_hip.update_layout(title='é«‹å…³èŠ‚è§’åº¦å˜åŒ–', xaxis_title='å¸§å·', yaxis_title='è§’åº¦ (Â°)', template='plotly_dark')
                            st.plotly_chart(fig_hip, use_container_width=True)
                            st.dataframe(df)
                    
                    if collected_messages:
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½æœ¬æ¬¡åˆ†ææŠ¥å‘Š",
                            data=collected_messages,
                            file_name=f"ai_coach_report_{time.strftime('%Y%m%d-%H%M%S')}.md",
                            mime="text/markdown",
                        )
                st.success("åˆ†æå®Œæˆï¼")
            except Exception as e:
                error_str = str(e)
                if "safety" in error_str.lower() or "blocked" in error_str.lower():
                     st.error("è¯·æ±‚è¢«å®‰å…¨ç­–ç•¥é˜»æ­¢ï¼Œå¯èƒ½æ˜¯å›¾åƒæˆ–æ–‡æœ¬å†…å®¹è¢«è¯¯åˆ¤ã€‚è¯·å°è¯•æ›´æ¢è§†é¢‘æˆ–è°ƒæ•´æç¤ºã€‚")
                else:
                     st.error(f"è°ƒç”¨AIæ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                print(e)

# ä»…åœ¨AIæœ‰å›å¤åï¼ˆå³åˆ†æå®Œæˆåï¼‰æ˜¾ç¤ºè¾“å…¥æ¡†
if st.session_state.history and isinstance(st.session_state.history[-1], AIMessage):
    if prompt := st.chat_input('å¯ä»¥ç»§ç»­å‘AIæé—®ï¼Œä¾‹å¦‚"æˆ‘çš„å·¦è…¿åº”è¯¥æ³¨æ„ä»€ä¹ˆï¼Ÿ"'):
        user_prompt_message = HumanMessage(content=prompt)
        st.session_state.history.append(user_prompt_message)
        
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            response_container = st.empty()
            collected_messages = ""
            
            # --- RAG + Chat é€»è¾‘ ---
            # å¦‚æœRAGå¯ç”¨ï¼Œåˆ™ä½¿ç”¨RAGé“¾ï¼›å¦åˆ™é€€å›æ™®é€šå¯¹è¯
            if st.session_state.retriever:
                rag_prompt_template = ChatPromptTemplate.from_template("""
                **è¯·æ³¨æ„ï¼š** ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIè¿åŠ¨æ•™ç»ƒã€‚è¯·ä¸¥æ ¼æ ¹æ®ä¸‹é¢æä¾›çš„"çŸ¥è¯†åº“ä¸Šä¸‹æ–‡"æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
                å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼Œè¯·ç¤¼è²Œåœ°å‘ŠçŸ¥ç”¨æˆ·"æ ¹æ®æˆ‘ç°æœ‰çš„çŸ¥è¯†ï¼Œæˆ‘è¿˜æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"ï¼Œä¸è¦å°è¯•ç¼–é€ ç­”æ¡ˆã€‚
                ä½ çš„æ‰€æœ‰å›ç­”éƒ½å¿…é¡»ä½¿ç”¨ç®€ä½“ä¸­æ–‡ã€‚

                **çŸ¥è¯†åº“ä¸Šä¸‹æ–‡:**
                {context}

                **ç”¨æˆ·é—®é¢˜:**
                {input}
                """)
                
                document_chain = create_stuff_documents_chain(st.session_state.llm, rag_prompt_template)
                retrieval_chain = create_retrieval_chain(st.session_state.retriever, document_chain)
                
                # æµå¼ä¼ è¾“RAGé“¾çš„å“åº”
                response_stream = retrieval_chain.stream({"input": prompt, "history": st.session_state.history})
                
                for chunk in response_stream:
                    if "answer" in chunk:
                        collected_messages += chunk["answer"]
                        response_container.markdown(collected_messages, unsafe_allow_html=True)
            else:
                 # RAGä¸å¯ç”¨æ—¶çš„æ™®é€šå¯¹è¯
                response = st.session_state.llm.stream(st.session_state.history)
                for chunk in response:
                    if chunk.content:
                        collected_messages += chunk.content
                        response_container.markdown(collected_messages, unsafe_allow_html=True)
            
            # å°†åç»­å›å¤ä¹ŸåŠ å…¥å†å²
            st.session_state.history.append(AIMessage(content=collected_messages))

