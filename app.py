# å…ˆåˆ‡æ¢ç«¯å£
# set HTTPS_PROXY=http://127.0.0.1:7897 ä¸€å®šè¦åœ¨cmdé‡Œåˆ‡æ¢åˆ°ä½ vpnçš„ç«¯å£ï¼Œä¸ç„¶è¿ä¸ä¸Šã€‚ä¸€å®šè¦åœ¨cmdé‡Œï¼ï¼ï¼
# streamlit run app.py æ¥å¯åŠ¨
# --- æœ€ç»ˆä¼˜åŒ–ä¸ä¿®æ­£ç‰ˆæœ¬ v4 ---

import streamlit as st
import google.generativeai as genai
import cv2
import mediapipe as mp
import tempfile
import time
from PIL import Image
import io
import numpy as np
import pandas as pd
import plotly.graph_objs as go
import re

# --- é¡µé¢é…ç½®å’Œæ ‡é¢˜ ---
st.set_page_config(
    page_title="AIè¿åŠ¨æ•™ç»ƒ Demo",
    page_icon="ğŸ¤–",
    layout="wide"
)
st.title("ğŸ¤– AI è¿åŠ¨æ•™ç»ƒ")
st.caption("ä¸Šä¼ ä¸€æ®µè¿åŠ¨è§†é¢‘ï¼Œè®©AIä¸ºä½ åˆ†æå§¿æ€ã€‚æ”¯æŒè¿ç»­å¯¹è¯ã€‚")


# --- Gemini API é…ç½® (è‡ªåŠ¨ä» secrets è¯»å–) ---
api_key = st.secrets.get('GEMINI_API_KEY', None)
if not api_key:
    st.error("æœªæ£€æµ‹åˆ° Gemini API å¯†é’¥ï¼Œè¯·åœ¨ .streamlit/secrets.toml ä¸­é…ç½® GEMINI_API_KEYã€‚")
    st.stop()

try:
    genai.configure(api_key=api_key)
except Exception as e:
    st.error(f"APIå¯†é’¥é…ç½®å¤±è´¥: {e}")
    st.stop()


# --- åˆå§‹åŒ– MediaPipe ---
mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils

# --- å®šä¹‰å®‰å…¨è®¾ç½® ---
safety_settings = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]

# --- åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ (å®ç°è®°å¿†åŠŸèƒ½) ---
try:
    if "model" not in st.session_state:
        st.session_state.model = genai.GenerativeModel(
            'gemini-1.5-flash',
            safety_settings=safety_settings
        )
    if "chat" not in st.session_state:
        st.session_state.chat = st.session_state.model.start_chat(history=[])
except Exception as e:
    st.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥API Keyå’Œç½‘ç»œä»£ç†ã€‚é”™è¯¯: {e}")
    st.stop()


# --- ä¾§è¾¹æ  UI ---
with st.sidebar:
    st.header("âš™ï¸ æ§åˆ¶é¢æ¿") # ä¼˜åŒ–ç‚¹ï¼šä½¿ç”¨æ›´é€šç”¨çš„å›¾æ ‡
    
    # --- ä¼˜åŒ–ç‚¹ï¼šæ”¹è¿›"æ–°å»ºå¯¹è¯"äº¤äº’ ---
    if st.button("âœ¨ æ–°å»ºå¯¹è¯", use_container_width=True):
        st.session_state.chat = st.session_state.model.start_chat(history=[])
        st.success("æ–°çš„å¯¹è¯å·²å¼€å§‹ï¼")
        time.sleep(0.5) # çŸ­æš‚æ˜¾ç¤ºæˆåŠŸä¿¡æ¯ï¼Œç„¶ååˆ·æ–°
        st.rerun()

    # --- é“œç‰ŒåŠŸèƒ½ï¼šå¢åŠ ç”¨æˆ·ç›®æ ‡è¾“å…¥æ¡† ---
    user_goal = st.text_input("æˆ‘çš„è®­ç»ƒç›®æ ‡:", placeholder="ä¾‹å¦‚ï¼šæ”¹å–„æ·±è¹²æ—¶è†ç›–å†…æ‰£")
    st.divider()

    # --- ä¼˜åŒ–ç‚¹ï¼šå¢åŠ ç”¨æˆ·å¼•å¯¼ ---
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ ä½ çš„è¿åŠ¨è§†é¢‘",
        type=["mp4", "mov", "avi"],
        help="å»ºè®®ä¸Šä¼ 5-15ç§’çš„çŸ­è§†é¢‘ï¼Œä»¥è·å¾—æœ€ä½³åˆ†æé€Ÿåº¦å’Œæ•ˆæœã€‚"
    )
    
    desired_frames = st.slider(
        "åˆ†æå¼ºåº¦ (å¸§æ•°)", # ä¼˜åŒ–ç‚¹ï¼šæ ‡ç­¾æ›´æ˜“æ‡‚
        min_value=2, max_value=10, value=6, step=1,
        help="é€‰æ‹©ä»è§†é¢‘ä¸­æŠ½å–çš„å…³é”®ç”»é¢æ•°é‡ã€‚æ•°é‡è¶Šå¤šï¼Œåˆ†æè¶Šç²¾ç»†ï¼Œä½†å¤„ç†æ—¶é—´ä¹Ÿæ›´é•¿ã€‚"
    )
    
    analyze_button = st.button("å¼€å§‹åˆ†æ", use_container_width=True, disabled=not uploaded_file)


# --- ä¸»èŠå¤©ç•Œé¢ ---

# --- ä¼˜åŒ–ç‚¹ï¼šå¢åŠ æ¬¢è¿é¡µ/å¼•å¯¼åŒºï¼Œé¿å…å†·å¯åŠ¨ ---
if not st.session_state.chat.history:
    st.markdown("""
        <div style="text-align: center; padding: 2rem 1rem;">
            <h2 style="font-weight: bold;">æ¬¢è¿ä½¿ç”¨ AI è¿åŠ¨æ•™ç»ƒ</h2>
            <p>æˆ‘æ˜¯æ‚¨çš„ä¸“å±AIæ•™ç»ƒï¼Œå¯ä»¥åˆ†ææ‚¨ä¸Šä¼ çš„è¿åŠ¨è§†é¢‘ï¼Œæä¾›ä¸“ä¸šçš„å§¿æ€è¯„ä¼°å’Œæ”¹è¿›å»ºè®®ã€‚</p>
            <p><strong>è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤å¼€å§‹ï¼š</strong></p>
            <ol style="display: inline-block; text-align: left; margin-top: 1rem;">
                <li>åœ¨å·¦ä¾§çš„ <strong>æ§åˆ¶é¢æ¿</strong> ä¸Šä¼ æ‚¨çš„è¿åŠ¨è§†é¢‘ã€‚</li>
                <li>ï¼ˆå¯é€‰ï¼‰è°ƒæ•´æ‚¨å¸Œæœ›åˆ†æçš„ <strong>å…³é”®å¸§æ•°é‡</strong>ã€‚</li>
                <li>ç‚¹å‡» <strong>"å¼€å§‹åˆ†æ"</strong> æŒ‰é’®ï¼Œç¨ç­‰ç‰‡åˆ»å³å¯è·å¾—æŠ¥å‘Šã€‚</li>
            </ol>
            <p>æœŸå¾…çœ‹åˆ°æ‚¨çš„ç²¾å½©è¡¨ç°ï¼</p>
        </div>
    """, unsafe_allow_html=True)

# æ˜¾ç¤ºå†å²å¯¹è¯è®°å½•
for message in st.session_state.chat.history:
    if hasattr(message, 'role') and message.role in ['user', 'model', 'assistant']:
        role = "assistant" if message.role == 'model' else message.role
        with st.chat_message(role):
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹è½½æŒ‰é’®çš„å…ƒæ•°æ®ï¼Œé¿å…é‡å¤æ˜¾ç¤º
            if "download" in message.parts[-1].text:
                 # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„å¤„ç†ï¼Œå®é™…åº”ç”¨å¯èƒ½éœ€è¦æ›´ç¨³å¥çš„å…ƒæ•°æ®æ–¹æ¡ˆ
                 st.markdown(message.parts[0].text, unsafe_allow_html=True)
                 st.download_button(
                     label="ğŸ“¥ ä¸‹è½½æœ¬æ¬¡åˆ†ææŠ¥å‘Š",
                     data=message.parts[0].text,
                     file_name=f"ai_coach_report_{time.strftime('%Y%m%d-%H%M%S')}.md",
                     mime="text/markdown",
                 )
            else:
                for part in message.parts:
                    if part.text:
                        st.markdown(part.text, unsafe_allow_html=True)

# --- é‡‘ç‰ŒåŠŸèƒ½ï¼šæ–°å¢è§’åº¦è®¡ç®—å‡½æ•° ---
def calculate_angle(a, b, c):
    """è®¡ç®—ç”±ä¸‰ç‚¹a, b, cæ„æˆçš„è§’åº¦ï¼ˆbä¸ºé¡¶ç‚¹ï¼‰ï¼Œè¿”å›0-180ä¹‹é—´çš„è§’åº¦å€¼"""
    a = np.array(a)
    b = np.array(b)
    c = np.array(c)
    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])
    angle = np.abs(radians * 180.0 / np.pi)
    if angle > 180.0:
        angle = 360 - angle
    return angle

# --- æ ¸å¿ƒå¤„ç†é€»è¾‘ ---
if analyze_button:
    # --- é‡‘ç‰ŒåŠŸèƒ½ï¼šåˆå§‹åŒ–ç”¨äºå­˜å‚¨é‡åŒ–æ•°æ®çš„å­—å…¸ ---
    quantitative_data = {
        "å¸§å·": [],
        "å·¦è†è§’åº¦": [],
        "å³è†è§’åº¦": [],
        "å·¦é«‹è§’åº¦": [],
        "å³é«‹è§’åº¦": []
    }
    with st.spinner("å¤„ç†ä¸­ï¼Œè¯·ç¨å€™..."):
        st.info("AIæ­£åœ¨åˆ†æ...è¯·ç¨å€™")
        # --- è§†é¢‘å¤„ç†é€»è¾‘ ---
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tfile:
            tfile.write(uploaded_file.read())
            video_path = tfile.name
        
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        if total_frames == 0:
            st.error("æ— æ³•è¯»å–è§†é¢‘ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æŸåã€‚")
            st.stop()
        
        frame_interval = max(total_frames // desired_frames, 1)
        st.write(f"è§†é¢‘æ€»å¸§æ•°: {total_frames}ï¼Œå°†å‡åŒ€æŠ½å– {desired_frames} å¸§è¿›è¡Œåˆ†æã€‚")
        
        sampled_frames_pil = []
        frame_indices_to_extract = [i * frame_interval for i in range(desired_frames)]
        
        with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:
            for frame_index in frame_indices_to_extract:
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)
                ret, frame = cap.read()
                if not ret:
                    continue
                
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = pose.process(rgb_frame)
                annotated_image = frame.copy()
                
                if results.pose_landmarks:
                    mp_drawing.draw_landmarks(
                        annotated_image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,
                        landmark_drawing_spec=mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),
                        connection_drawing_spec=mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)
                    )
                
                # --- é‡‘ç‰ŒåŠŸèƒ½ï¼šè®¡ç®—è§’åº¦å¹¶å­˜å‚¨ ---
                try:
                    landmarks = results.pose_landmarks.landmark
                    left_hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]
                    left_knee = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]
                    left_ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]
                    left_shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]
                    right_hip = [landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y]
                    right_knee = [landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y]
                    right_ankle = [landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y]
                    right_shoulder = [landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y]
                    lk_angle = calculate_angle(left_hip, left_knee, left_ankle)
                    rk_angle = calculate_angle(right_hip, right_knee, right_ankle)
                    lh_angle = calculate_angle(left_shoulder, left_hip, left_knee)
                    rh_angle = calculate_angle(right_shoulder, right_hip, right_knee)
                    quantitative_data["å¸§å·"].append(frame_index)
                    quantitative_data["å·¦è†è§’åº¦"].append(lk_angle)
                    quantitative_data["å³è†è§’åº¦"].append(rk_angle)
                    quantitative_data["å·¦é«‹è§’åº¦"].append(lh_angle)
                    quantitative_data["å³é«‹è§’åº¦"].append(rh_angle)
                except Exception as e:
                    print(f"åœ¨å¸§ {frame_index} è®¡ç®—è§’åº¦æ—¶å‡ºé”™: {e}")
                    pass
                
                pil_image = Image.fromarray(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))
                sampled_frames_pil.append(pil_image)
        
        cap.release()
        st.write(f"å›¾åƒæå–å®Œæˆï¼å…±æå– {len(sampled_frames_pil)} å¸§ã€‚æ­£åœ¨å‡†å¤‡å‘é€...")

        if not sampled_frames_pil:
            st.error("æ— æ³•ä»è§†é¢‘ä¸­æå–ä»»ä½•æœ‰æ•ˆå¸§ã€‚")
        else:
            df = pd.DataFrame(quantitative_data)
            # --- åä¸½å…³é”®å¸§æ¨ªå‘å¤§å›¾å±•ç¤º ---
            st.markdown("#### å…³é”®å¸§é¢„è§ˆ")
            cols = st.columns(len(sampled_frames_pil))
            for i, img in enumerate(sampled_frames_pil):
                with cols[i]:
                    st.image(img, caption=f"å¸§ {i+1}", use_container_width=True)
            # --- ä¼˜åŒ–promptï¼Œå¼ºè°ƒå¤šæ¨¡æ€ç†è§£ï¼Œé‡åŒ–æ•°æ®ä»…ä¸ºè¾…åŠ© ---
            focus_prompt = f"ç”¨æˆ·çš„è®­ç»ƒç›®æ ‡æ˜¯{user_goal}ã€‚å¦‚æœ‰ç›¸å…³é—®é¢˜è¯·é€‚å½“å…³æ³¨ã€‚" if user_goal else ""
            data_prompt = f"\nä»¥ä¸‹ä¸ºéƒ¨åˆ†å¸§çš„é‡åŒ–æ•°æ®ï¼Œä»…ä¾›ä½ åˆ†ææ—¶å‚è€ƒï¼Œé‡ç‚¹è¯·ç»“åˆè§†é¢‘å¸§çš„å¤šæ¨¡æ€ç†è§£è¿›è¡Œç»¼åˆåˆ¤æ–­ï¼š\n{df.to_markdown(index=False)}\n" if not df.empty else ""
            prompt_text = f"""
            ä½ æ˜¯ä¸€ä½é¡¶çº§çš„è¿åŠ¨ç”Ÿç‰©åŠ›å­¦ä¸“å®¶å’ŒAIæ•™ç»ƒã€‚ä½ çš„æ‰€æœ‰å›ç­”éƒ½å¿…é¡»ä½¿ç”¨ç®€ä½“ä¸­æ–‡ã€‚
            ä½ çš„ä»»åŠ¡æ˜¯åŸºäºç”¨æˆ·ä¸Šä¼ çš„è§†é¢‘å¸§ä¸ºä¸»ï¼Œç»“åˆéƒ¨åˆ†é‡åŒ–æ•°æ®ï¼ˆä»…ä½œè¾…åŠ©å‚è€ƒï¼‰ï¼Œæä¾›ä¸€ä»½ä¸“ä¸šã€æ·±å…¥ã€ä»¥å¤šæ¨¡æ€ç†è§£ä¸ºæ ¸å¿ƒçš„åˆ†ææŠ¥å‘Šã€‚
            {focus_prompt}
            {data_prompt}

            **è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š**
            è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ä¸‰ä¸ªéƒ¨åˆ†è¿›è¡Œç»„ç»‡ï¼š

            - **ã€ç»¼åˆè¯„ä¼°ä¸å¾—åˆ†ã€‘**: 
              é¦–å…ˆï¼Œ**å¿…é¡»ä½¿ç”¨Markdownè¡¨æ ¼**æ¸…æ™°åœ°å±•ç¤ºå››ä¸ªç»´åº¦çš„å¾—åˆ†ã€‚è¡¨æ ¼åº”åŒ…å«"è¯„ä¼°ç»´åº¦"å’Œ"å¾—åˆ† (æ»¡åˆ†10)"ä¸¤åˆ—ã€‚
              ç„¶åï¼Œåœ¨è¡¨æ ¼ä¸‹æ–¹ç»™å‡ºä¸€ä¸ªç®€çŸ­çš„æ€»ä½“è¯„ä»·ã€‚

            - **ã€å¤šæ¨¡æ€è¯Šæ–­ã€‘**: 
              è¿™æ˜¯æŠ¥å‘Šçš„æ ¸å¿ƒã€‚è¯·**ä»¥è§†é¢‘å¸§çš„å¤šæ¨¡æ€ç†è§£ä¸ºä¸»ï¼Œé‡åŒ–æ•°æ®ä»…ä½œè¾…åŠ©**ï¼Œé€é¡¹è§£é‡Šæ‰“åˆ†ä¾æ®ã€‚
              ä¾‹å¦‚ï¼š"åœ¨ç¬¬Xå¸§å›¾åƒä¸­è§‚å¯Ÿåˆ°..."ã€‚ä¸è¦åªå›´ç»•é‡åŒ–æ•°æ®å±•å¼€ã€‚

            - **ã€æ ¸å¿ƒæ”¹è¿›å»ºè®®ã€‘**: 
              é’ˆå¯¹å¾—åˆ†è¾ƒä½çš„ç»´åº¦å’Œç”¨æˆ·ç›®æ ‡ï¼Œæä¾›æœ€å…³é”®ã€æœ€å¯æ“ä½œçš„è®­ç»ƒå»ºè®®ã€‚

            ä½ çš„è¯­æ°”åº”ä¸“ä¸šã€ä¸¥è°¨ä¸”å¯Œæœ‰é¼“åŠ±æ€§ã€‚è¯·å¼€å§‹åˆ†æã€‚
            """
            # ä¿æŒfull_promptä¸ºå›¾ç‰‡å†…å®¹
            image_parts = []
            for img in sampled_frames_pil:
                img.thumbnail((768, 768))
                buf = io.BytesIO()
                img.save(buf, format='JPEG')
                image_parts.append({"mime_type": "image/jpeg", "data": buf.getvalue()})
            full_prompt = [prompt_text] + image_parts
            with st.chat_message("user"):
                pass
            try:
                st.write("æ­£åœ¨åˆ†æ...")
                response = st.session_state.chat.send_message(full_prompt, stream=True)
                with st.chat_message("assistant"):
                    response_container = st.empty()
                    collected_messages = ""
                    for chunk in response:
                        if chunk.text:
                            collected_messages += chunk.text
                            response_container.markdown(collected_messages, unsafe_allow_html=True)
                    # --- plotlyæŠ˜çº¿å›¾ ---
                    if not df.empty:
                        with st.expander("ğŸ“ˆ è¯¦ç»†æ•°æ®å›¾è¡¨", expanded=True):
                            # è†å…³èŠ‚è§’åº¦å˜åŒ–
                            fig_knee = go.Figure()
                            fig_knee.add_trace(go.Scatter(x=df['å¸§å·'], y=df['å·¦è†è§’åº¦'], mode='lines+markers', name='å·¦è†', line=dict(color='red', width=4), marker=dict(size=10)))
                            fig_knee.add_trace(go.Scatter(x=df['å¸§å·'], y=df['å³è†è§’åº¦'], mode='lines+markers', name='å³è†', line=dict(color='blue', width=4), marker=dict(size=10)))
                            fig_knee.update_layout(title='è†å…³èŠ‚è§’åº¦å˜åŒ–', xaxis_title='å¸§å·', yaxis_title='è§’åº¦ (Â°)', template='plotly_dark')
                            st.plotly_chart(fig_knee, use_container_width=True)
                            # é«‹å…³èŠ‚è§’åº¦å˜åŒ–
                            fig_hip = go.Figure()
                            fig_hip.add_trace(go.Scatter(x=df['å¸§å·'], y=df['å·¦é«‹è§’åº¦'], mode='lines+markers', name='å·¦é«‹', line=dict(color='orange', width=4), marker=dict(size=10)))
                            fig_hip.add_trace(go.Scatter(x=df['å¸§å·'], y=df['å³é«‹è§’åº¦'], mode='lines+markers', name='å³é«‹', line=dict(color='green', width=4), marker=dict(size=10)))
                            fig_hip.update_layout(title='é«‹å…³èŠ‚è§’åº¦å˜åŒ–', xaxis_title='å¸§å·', yaxis_title='è§’åº¦ (Â°)', template='plotly_dark')
                            st.plotly_chart(fig_hip, use_container_width=True)
                            st.dataframe(df)
                    # --- ä¸‹è½½æŠ¥å‘Š ---
                    if collected_messages:
                        st.download_button(
                            label="ğŸ“¥ ä¸‹è½½æœ¬æ¬¡åˆ†ææŠ¥å‘Š",
                            data=collected_messages,
                            file_name=f"ai_coach_report_{time.strftime('%Y%m%d-%H%M%S')}.md",
                            mime="text/markdown",
                        )
                st.success("åˆ†æå®Œæˆï¼")
            except genai.types.BlockedPromptError as e:
                st.error("è¯·æ±‚è¢«å®‰å…¨ç­–ç•¥é˜»æ­¢ï¼Œå¯èƒ½æ˜¯å›¾åƒæˆ–æ–‡æœ¬å†…å®¹è¢«è¯¯åˆ¤ã€‚")
                print(e)
            except Exception as e:
                st.error(f"è°ƒç”¨MLLMæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                print(e)

# ä»…åœ¨AIå›å¤åæ˜¾ç¤ºè¾“å…¥æ¡†
if st.session_state.chat.history and st.session_state.chat.history[-1].role == 'model':
    if prompt := st.chat_input('å¯ä»¥ç»§ç»­å‘AIæé—®ï¼Œä¾‹å¦‚"æˆ‘çš„å·¦è…¿åº”è¯¥æ³¨æ„ä»€ä¹ˆï¼Ÿ"'):
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            response_container = st.empty()
            collected_messages = ""
            # æµå¼ä¼ è¾“æ¥è‡ªåç»­èŠå¤©æ¶ˆæ¯çš„å“åº”
            response = st.session_state.chat.send_message(prompt, stream=True)
            for chunk in response:
                if chunk.text:
                    collected_messages += chunk.text
                    response_container.markdown(collected_messages, unsafe_allow_html=True)

